{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b9cc808"
   },
   "source": [
    "# Deep Learning Example - Iris \n",
    "\n",
    "This examples demonstrates the core deep learning model building concepts using the Keras library. The Iris flower dataset is used to build the model and perform classification tasks\n",
    "\n",
    "Dataset Feature Variables: Sepal length, Sepal width, Petal length, Petal width\n",
    "\n",
    "Target Class: Species ( Setosa, Versicolor, Virginica)\n",
    "\n",
    "150 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7141cfab"
   },
   "source": [
    "### 5.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17aae7a8",
    "outputId": "1758c220-b0bb-4acf-d69b-9784aa06b776"
   },
   "outputs": [],
   "source": [
    "#Install related libraries for the course. \n",
    "#This is a common requirement for all other exampels too\n",
    "\n",
    "!pip install pandas\n",
    "!pip install tensorflow\n",
    "!pip install sklearn\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fabf059"
   },
   "source": [
    "### 4.2. Prepare Input Data for Deep Learning\n",
    "\n",
    "Perform the following steps for preparing data\n",
    "\n",
    "1. Load data into a pandas dataframe\n",
    "2. Convert the dataframe to a numpy array\n",
    "3. Scale the feature dataset\n",
    "4. Use one-hot-encoding for the target variable\n",
    "5. Split into training and test datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6db4bd81",
    "outputId": "dccd16d3-e4dc-472b-c2de-650ac5eadbdc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Load Data and review content\n",
    "iris_data = pd.read_csv(\"iris.csv\")\n",
    "\n",
    "print(\"\\nLoaded Data :\\n------------------------------------\")\n",
    "print(iris_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CyOkvKGE814W",
    "outputId": "a28009c6-6017-4d73-c5ad-d76af2a96c40"
   },
   "outputs": [],
   "source": [
    "#Use a Label encoder to convert String to numeric values \n",
    "#for the target variable\n",
    "\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "iris_data['Species'] = label_encoder.fit_transform(\n",
    "                                iris_data['Species'])\n",
    "\n",
    "#Convert input to numpy array\n",
    "np_iris = iris_data.to_numpy()\n",
    "\n",
    "#Separate feature and target variables\n",
    "X_data = np_iris[:,0:4]\n",
    "Y_data=np_iris[:,4]\n",
    "\n",
    "print(\"\\nFeatures before scaling :\\n------------------------------------\")\n",
    "print(X_data[:5,:])\n",
    "print(\"\\nTarget before scaling :\\n------------------------------------\")\n",
    "print(Y_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEVpfTJu8fR7",
    "outputId": "b1f9d1f6-311d-430c-d2b2-c482098287a1"
   },
   "outputs": [],
   "source": [
    "#Create a scaler model that is fit on the input data.\n",
    "scaler = StandardScaler().fit(X_data)\n",
    "\n",
    "#Scale the numeric feature variables\n",
    "X_data = scaler.transform(X_data)\n",
    "\n",
    "#Convert target variable as a one-hot-encoding array\n",
    "Y_data = tf.keras.utils.to_categorical(Y_data,3)\n",
    "\n",
    "print(\"\\nFeatures after scaling :\\n------------------------------------\")\n",
    "print(X_data[:5,:])\n",
    "print(\"\\nTarget after one-hot-encoding :\\n------------------------------------\")\n",
    "print(Y_data[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m8Dd8JSf9Fu_",
    "outputId": "2c1a6d92-04d9-46d8-c525-7fd0c034f436"
   },
   "outputs": [],
   "source": [
    "#Split training and test data\n",
    "X_train,X_test,Y_train,Y_test = train_test_split( X_data, Y_data, test_size=0.10)\n",
    "\n",
    "print(\"\\nTrain Test Dimensions:\\n------------------------------------\")\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bb5fad2"
   },
   "source": [
    "### 4.3. Creating a Model\n",
    "\n",
    "Creating a model in Keras requires defining the following\n",
    "\n",
    "1. Number of hidden layers\n",
    "2. Number of nodes in each layer\n",
    "3. Activation functions\n",
    "4. Loss Function & Accuracy measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4a0be90",
    "outputId": "2929f55b-0cc1-4552-8dc0-bb70b26bb7ca"
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "#Number of classes in the target variable\n",
    "NB_CLASSES=3\n",
    "\n",
    "#Create a sequencial model in Keras\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Add the first hidden layer\n",
    "model.add(keras.layers.Dense(128,                    #Number of nodes\n",
    "                             input_shape=(4,),       #Number of input variables\n",
    "                              name='Hidden-Layer-1', #Logical name\n",
    "                              activation='relu'))    #activation function\n",
    "\n",
    "#Add a second hidden layer\n",
    "model.add(keras.layers.Dense(128,\n",
    "                              name='Hidden-Layer-2',\n",
    "                              activation='relu'))\n",
    "\n",
    "#Add an output layer with softmax activation\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "                             name='Output-Layer',\n",
    "                             activation='softmax'))\n",
    "\n",
    "#Compile the model with loss & metrics\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Print the model meta-data\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95c6677e"
   },
   "source": [
    "### 4.4. Training and evaluating the Model\n",
    "\n",
    "Training the model involves defining various training models and then perform \n",
    "forward and back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "55a9ddba",
    "outputId": "99113621-1d8a-46d9-ac32-0f0fe9bdb641"
   },
   "outputs": [],
   "source": [
    "#Make it verbose so we can see the progress\n",
    "VERBOSE=1\n",
    "\n",
    "#Setup Hyper Parameters for training\n",
    "\n",
    "#Set Batch size\n",
    "BATCH_SIZE=16\n",
    "#Set number of epochs\n",
    "EPOCHS=10\n",
    "#Set validation split. 20% of the training data will be used for validation\n",
    "#after each epoch\n",
    "VALIDATION_SPLIT=0.2\n",
    "\n",
    "print(\"\\nTraining Progress:\\n------------------------------------\")\n",
    "\n",
    "#Fit the model. This will perform the entire training cycle, including\n",
    "#forward propagation, loss computation, backward propagation and gradient descent.\n",
    "#Execute for the specified batch sizes and epoch\n",
    "#Perform validation after each epoch \n",
    "history=model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=VERBOSE,\n",
    "          validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "print(\"\\nAccuracy during Training :\\n------------------------------------\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot accuracy of the model after each epoch.\n",
    "pd.DataFrame(history.history)[\"accuracy\"].plot(figsize=(8, 5))\n",
    "plt.title(\"Accuracy improvements with Epoch\")\n",
    "plt.show()\n",
    "\n",
    "#Evaluate the model against the test dataset and print results\n",
    "print(\"\\nEvaluation against Test Dataset :\\n------------------------------------\")\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55efdff7"
   },
   "source": [
    "### 4.5. Saving and Loading Models\n",
    "\n",
    "The training and inference environments are usually separate. Models need to be saved after they are validated. They are then loaded into the inference environments for actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7434d7cb",
    "outputId": "3b34f645-3432-46be-ba3b-f7ac8041f418"
   },
   "outputs": [],
   "source": [
    "#Saving a model\n",
    "    \n",
    "model.save(\"iris_save\")\n",
    "    \n",
    "#Loading a Model \n",
    "loaded_model = keras.models.load_model(\"iris_save\")\n",
    "\n",
    "#Print Model Summary\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6cc6fb5"
   },
   "source": [
    "### 4.6. Predictions with Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58037d5d",
    "outputId": "db5d95ae-65cf-4139-c5e2-a41c73d3a28b"
   },
   "outputs": [],
   "source": [
    "#Raw prediction data\n",
    "prediction_input = [[6.6, 3. , 4.4, 1.4]]\n",
    "\n",
    "#Scale prediction data with the same scaling model\n",
    "scaled_input = scaler.transform(prediction_input)\n",
    "\n",
    "#Get raw prediction probabilities\n",
    "raw_prediction = model.predict(scaled_input)\n",
    "print(\"Raw Prediction Output (Probabilities) :\" , raw_prediction)\n",
    "\n",
    "#Find prediction\n",
    "prediction = np.argmax(raw_prediction)\n",
    "print(\"Prediction is \", label_encoder.inverse_transform([prediction]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dc76d3ca"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
